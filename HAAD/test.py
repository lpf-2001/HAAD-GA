# main_cross_model_attack.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
import json
import os
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from Ant_algorithm import UniversalHAAD
import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from DLWF_pytorch.model.model_5000 import *
from utils.data import load_rimmer_dataset


class CrossModelAttackEvaluator:
    """è·¨æ¨¡å‹æ”»å‡»è¯„ä¼°å™¨"""
    
    def __init__(self, device: Optional[torch.device] = None):
        if device is None:
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.device = device
        
    def evaluate_model_performance(
        self, 
        model: nn.Module, 
        data: torch.Tensor, 
        labels: torch.Tensor,
        batch_size: int = 512,
        model_name: str = "Model"
    ) -> Dict:
        """è¯„ä¼°æ¨¡å‹åœ¨æ•°æ®ä¸Šçš„æ€§èƒ½"""
        model.eval()
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        predictions = []
        confidences = []
        
        with torch.no_grad():
            for i in range(0, len(data), batch_size):
                batch_data = data[i:i+batch_size].to(self.device)
                batch_labels = labels[i:i+batch_size].to(self.device)
                
                if batch_labels.ndim > 1 and batch_labels.size(-1) > 1:
                    targets = batch_labels.argmax(dim=-1)
                else:
                    targets = batch_labels.long()
                
                outputs = model(batch_data)
                loss = F.cross_entropy(outputs, targets, reduction='sum')
                
                probs = F.softmax(outputs, dim=-1)
                pred_classes = outputs.argmax(dim=-1)
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                total_correct += (pred_classes == targets).sum().item()
                total_samples += batch_data.size(0)
                
                # æ”¶é›†é¢„æµ‹å’Œç½®ä¿¡åº¦
                predictions.extend(pred_classes.cpu().numpy())
                target_confidences = probs[torch.arange(len(targets)), targets]
                confidences.extend(target_confidences.cpu().numpy())
        
        accuracy = total_correct / total_samples
        avg_loss = total_loss / total_samples
        avg_confidence = np.mean(confidences)
        
        return {
            'model_name': model_name,
            'accuracy': accuracy,
            'avg_loss': avg_loss,
            'avg_confidence': avg_confidence,
            'total_samples': total_samples
            # 'predictions': np.array(predictions),
            # 'confidences': np.array(confidences)
        }
    
    def compare_performance(
        self, 
        original_results: Dict, 
        perturbed_results: Dict
    ) -> Dict:
        """æ¯”è¾ƒåŸå§‹å’Œæ‰°åŠ¨åçš„æ€§èƒ½"""
        return {
            'accuracy_drop': original_results['accuracy'] - perturbed_results['accuracy'],
            'loss_increase': perturbed_results['avg_loss'] - original_results['avg_loss'],
            'confidence_drop': original_results['avg_confidence'] - perturbed_results['avg_confidence'],
            'attack_success_rate': 1.0 - perturbed_results['accuracy'],
            'relative_accuracy_drop': (original_results['accuracy'] - perturbed_results['accuracy']) / original_results['accuracy'] if original_results['accuracy'] > 0 else 0
        }


def load_models(model_paths: Dict[str, str], device: torch.device) -> Dict[str, nn.Module]:
    """åŠ è½½æ¨¡å‹å­—å…¸"""
    models = {}
    for name, path in model_paths.items():
        if os.path.exists(path):
            model = torch.load(path, map_location=device)
            model.to(device)
            model.eval()
            models[name] = model
            print(f"âœ… åŠ è½½æ¨¡å‹ {name}: {path}")
        else:
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    return models


def save_results(results: Dict, save_path: str):
    """ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶"""
    # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
    def convert_for_json(obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, torch.Tensor):
            return obj.cpu().numpy().tolist()
        elif isinstance(obj, (np.floating,)):
            return float(obj)
        elif isinstance(obj, (np.integer,)):
            return int(obj)
        elif isinstance(obj, dict):
            return {k: convert_for_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_for_json(item) for item in obj]
        else:
            return obj

    
    json_results = convert_for_json(results)
    
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(json_results, f, indent=2, ensure_ascii=False)

    
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {save_path}")


def plot_results(results: Dict, save_dir: str):
    """ç»˜åˆ¶ç»“æœå¯è§†åŒ–å›¾è¡¨"""
    plt.style.use('default')
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # æå–æ•°æ®
    model_names = list(results['target_models'].keys())
    accuracy_drops = [results['target_models'][name]['comparison']['accuracy_drop'] 
                     for name in model_names]
    loss_increases = [results['target_models'][name]['comparison']['loss_increase'] 
                     for name in model_names]
    confidence_drops = [results['target_models'][name]['comparison']['confidence_drop'] 
                       for name in model_names]
    attack_success_rates = [results['target_models'][name]['comparison']['attack_success_rate'] 
                           for name in model_names]
    
    # 1. å‡†ç¡®ç‡ä¸‹é™
    axes[0, 0].bar(model_names, accuracy_drops, color='skyblue', alpha=0.7)
    axes[0, 0].set_title('Accuracy Drop by Model', fontsize=12, weight='bold')
    axes[0, 0].set_ylabel('Accuracy Drop')
    axes[0, 0].tick_params(axis='x', rotation=45)
    for i, v in enumerate(accuracy_drops):
        axes[0, 0].text(i, v + max(accuracy_drops)*0.01, f'{v:.3f}', ha='center')
    
    # 2. æŸå¤±å¢åŠ 
    axes[0, 1].bar(model_names, loss_increases, color='lightcoral', alpha=0.7)
    axes[0, 1].set_title('Loss Increase by Model', fontsize=12, weight='bold')
    axes[0, 1].set_ylabel('Loss Increase')
    axes[0, 1].tick_params(axis='x', rotation=45)
    for i, v in enumerate(loss_increases):
        axes[0, 1].text(i, v + max(loss_increases)*0.01, f'{v:.3f}', ha='center')
    
    # 3. ç½®ä¿¡åº¦ä¸‹é™
    axes[1, 0].bar(model_names, confidence_drops, color='lightgreen', alpha=0.7)
    axes[1, 0].set_title('Confidence Drop by Model', fontsize=12, weight='bold')
    axes[1, 0].set_ylabel('Confidence Drop')
    axes[1, 0].tick_params(axis='x', rotation=45)
    for i, v in enumerate(confidence_drops):
        axes[1, 0].text(i, v + max(confidence_drops)*0.01, f'{v:.3f}', ha='center')
    
    # 4. æ”»å‡»æˆåŠŸç‡
    axes[1, 1].bar(model_names, attack_success_rates, color='gold', alpha=0.7)
    axes[1, 1].set_title('Attack Success Rate by Model', fontsize=12, weight='bold')
    axes[1, 1].set_ylabel('Attack Success Rate')
    axes[1, 1].tick_params(axis='x', rotation=45)
    for i, v in enumerate(attack_success_rates):
        axes[1, 1].text(i, v + max(attack_success_rates)*0.01, f'{v:.3f}', ha='center')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    plot_path = os.path.join(save_dir, 'attack_results.png')
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {plot_path}")
    
    # ç»˜åˆ¶é€‚åº”åº¦å†å²
    if 'perturbation_info' in results and 'fitness_history' in results['perturbation_info']:
        plt.figure(figsize=(10, 6))
        fitness_history = results['perturbation_info']['fitness_history']
        plt.plot(fitness_history, 'b-', linewidth=2, alpha=0.8)
        plt.title('Fitness Evolution During ACO Optimization', fontsize=14, weight='bold')
        plt.xlabel('Iteration')
        plt.ylabel('Fitness Value')
        plt.grid(True, alpha=0.3)
        
        fitness_plot_path = os.path.join(save_dir, 'fitness_evolution.png')
        plt.savefig(fitness_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… é€‚åº”åº¦è¿›åŒ–å›¾å·²ä¿å­˜åˆ°: {fitness_plot_path}")


def main():
    """ä¸»å‡½æ•°ï¼šè·¨æ¨¡å‹é€šç”¨æ‰°åŠ¨æ”»å‡»å®éªŒ"""
    
    # ==================== é…ç½®å‚æ•° ====================
    config = {
        # æ•°æ®é…ç½®
        'data_path': '/home/xuke/lpf/HAAD-GA/utils/Dataset/Rimmer/tor_100w_2500tr.npz',  # ä½ çš„æ•°æ®æ–‡ä»¶è·¯å¾„
        'batch_size': 512,
        
        # æ¨¡å‹é…ç½® - æ›¿æ¢ä¸ºä½ çš„å®é™…æ¨¡å‹è·¯å¾„
        'source_model_path': '/home/xuke/lpf/HAAD-GA/DLWF_pytorch/trained_model/length_5000/Rimmer/llm_rimmer100.pth',  # æºæ¨¡å‹llmï¼ˆç”¨äºç”Ÿæˆæ‰°åŠ¨ï¼‰
        # 'source_model_path':'/home/xuke/lpf/HAAD-GA/DLWF_pytorch/trained_model/length_5000/Rimmer/df_rimmer100.pth',
        'target_models': {
            'df': '/home/xuke/lpf/HAAD-GA/DLWF_pytorch/trained_model/length_5000/Rimmer/df_rimmer100.pth',        # ç›®æ ‡æ¨¡å‹B
            'varcnn': '/home/xuke/lpf/HAAD-GA/DLWF_pytorch/trained_model/length_5000/Rimmer/varcnn_rimmer100.pth',        # ç›®æ ‡æ¨¡å‹Cï¼ˆå¯é€‰ï¼‰
            # 'llm':'/home/xuke/lpf/HAAD-GA/DLWF_pytorch/trained_model/length_5000/Rimmer/llm_rimmer100.pth',
        },
        
        # HAADç®—æ³•é…ç½®
        'haad_config': {
            'numant': 20,                    # èš‚èšæ•°é‡
            'max_inject': 200,                # æœ€å¤§æ‰°åŠ¨ä½ç½®æ•°
            'max_iter': 10,                 # æœ€å¤§è¿­ä»£æ•°
            'alpha': 1.0,                    # ä¿¡æ¯ç´ é‡è¦æ€§
            'beta': 2.0,                     # å¯å‘å¼ä¿¡æ¯é‡è¦æ€§
            'rho': 0.1,                      # ä¿¡æ¯ç´ è’¸å‘ç‡
            'inject_value': 1.0,             # æ‰°åŠ¨å€¼
            'perturb_mode': 'overwrite',     # æ‰°åŠ¨æ¨¡å¼
            'memory_limit_mb': 8192,         # æ˜¾å­˜é™åˆ¶
            'eval_chunk_size': 256,          # è¯„ä¼°æ‰¹é‡å¤§å°
        },
        
        # å®éªŒé…ç½®
        'fitness_type': 'accuracy_drop',     # é€‚åº”åº¦å‡½æ•°ç±»å‹
        'heuristic_sample_size': 2000,       # å¯å‘å¼ä¿¡æ¯é‡‡æ ·å¤§å°
        'pheromone_strategy': 'elitist',     # ä¿¡æ¯ç´ æ›´æ–°ç­–ç•¥
        
        # è¾“å‡ºé…ç½®
        'output_dir': 'results/cross_model_attack',
        'save_plots': True,
        'verbose': True
    }
    
    # ==================== åˆå§‹åŒ– ====================
    print("ğŸš€ å¼€å§‹è·¨æ¨¡å‹é€šç”¨æ‰°åŠ¨æ”»å‡»å®éªŒ")
    print("=" * 50)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = f"{config['output_dir']}_{timestamp}"
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = CrossModelAttackEvaluator(device)
    
    # ==================== åŠ è½½æ•°æ® ====================
    print("\nğŸ“ åŠ è½½æ•°æ®...")
    
    # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„æ•°æ®æ ¼å¼è¿›è¡Œè°ƒæ•´
    if os.path.exists(config['data_path']):
        # data_dict = torch.load(config['data_path'])
        train_data, train_labels, valid_data, valid_labels, test_data, test_labels = load_rimmer_dataset(input_size=5000,num_classes=100,test_ratio=0.1,val_ratio=0.1)
        train_data = torch.tensor(train_data, dtype=torch.float32)
        train_labels = torch.tensor(train_labels, dtype=torch.int)
        test_data = torch.tensor(test_data, dtype=torch.float32)
        test_labels = torch.tensor(test_labels, dtype=torch.int)
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"   è®­ç»ƒæ•°æ®: {train_data.shape}")
        print(f"   æµ‹è¯•æ•°æ®: {test_data.shape}")
    else:
        # ç¤ºä¾‹ï¼šç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        print("âš ï¸  æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
        B, L, C = 1000, 100, 1
        train_data = torch.randn(B, L, C)
        train_labels = torch.randint(0, 10, (B,))
        test_data = torch.randn(200, L, C)
        test_labels = torch.randint(0, 10, (200,))
    
    # ==================== åŠ è½½æ¨¡å‹ ====================
    print("\nğŸ¤– åŠ è½½æ¨¡å‹...")
    
    # åŠ è½½æºæ¨¡å‹Aï¼ˆç”¨äºç”Ÿæˆæ‰°åŠ¨ï¼‰
    if os.path.exists(config['source_model_path']):
        source_model =  MultiScaleLLM_V2(num_classes=100).to(device)  # æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹ç±»
        # source_model = DFNet(100).to(device)
        source_model.load_state_dict(torch.load(config['source_model_path'], map_location=device))
        source_model.to(device)
        source_model.eval()
        print(f"âœ… æºæ¨¡å‹AåŠ è½½æˆåŠŸ: {config['source_model_path']}")
    else:
        # ç¤ºä¾‹ï¼šåˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹
        print("âš ï¸  æºæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹")
        source_model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(train_data.shape[1] * train_data.shape[2], 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        ).to(device)
    
    # åŠ è½½ç›®æ ‡æ¨¡å‹Bã€Cã€Dç­‰
    target_models = {}
    for name, path in config['target_models'].items():
        if os.path.exists(path):
            if name == 'df':
                model = DFNet(100).to(device)
            elif name == 'varcnn':
                model = VarCNN(5000,100).to(device)
            elif name == 'llm':
                model = MultiScaleLLM_V2(num_classes=100).to(device)
            model.load_state_dict(torch.load(path, map_location=device))
            model.to(device)
            model.eval()
            target_models[name] = model
            print(f"âœ… ç›®æ ‡æ¨¡å‹{name}åŠ è½½æˆåŠŸ: {path}")

    
    # ==================== æ­¥éª¤1: ç”¨æºæ¨¡å‹Aç”Ÿæˆé€šç”¨æ‰°åŠ¨ ====================
    print("\nğŸœ æ­¥éª¤1: ä½¿ç”¨æºæ¨¡å‹Aè¿è¡Œèšç¾¤ç®—æ³•ç”Ÿæˆé€šç”¨æ‰°åŠ¨...")
    print("-" * 30)
    
    # åˆå§‹åŒ–HAADç®—æ³•
    haad = UniversalHAAD(
        model=source_model,
        original_trace=train_data,
        **config['haad_config']
    )
    
    # è¿è¡Œä¼˜åŒ–
    best_positions, best_fitness = haad.run(
        labels=train_labels,
        fitness_type=config['fitness_type'],
        heuristic_sample_size=config['heuristic_sample_size'],
        pheromone_strategy=config['pheromone_strategy'],
        verbose=config['verbose']
    )
    
    # è·å–æ‰°åŠ¨ä¿¡æ¯
    perturbation_info = haad.get_universal_perturbation_info()
    
    print(f"\nâœ… é€šç”¨æ‰°åŠ¨ç”Ÿæˆå®Œæˆ!")
    print(f"ğŸ¯ æ‰°åŠ¨ä½ç½®: {perturbation_info['perturbation_positions']}")
    print(f"ğŸ“Š æ‰°åŠ¨ä½ç½®æ•°é‡: {perturbation_info['num_positions']}")
    print(f"ğŸ“ˆ æœ€ç»ˆé€‚åº”åº¦: {perturbation_info['fitness']:.6f}")
    print(f"ğŸ“‰ æ‰°åŠ¨æ¯”ä¾‹: {perturbation_info['perturbation_ratio']:.2%}")
    
    # ==================== æ­¥éª¤2: åœ¨æºæ¨¡å‹Aä¸ŠéªŒè¯æ‰°åŠ¨æ•ˆæœ ====================
    print(f"\nğŸ” æ­¥éª¤2: åœ¨æºæ¨¡å‹Aä¸ŠéªŒè¯æ‰°åŠ¨æ•ˆæœ...")
    print("-" * 30)
    
    # åŸå§‹æ€§èƒ½
    source_original = evaluator.evaluate_model_performance(
        source_model, test_data, test_labels, 
        config['batch_size'], "Source_Model_A"
    )
    
    # æ‰°åŠ¨åæ€§èƒ½
    perturbed_test_data = haad.apply_to_new_data(test_data)
    source_perturbed = evaluator.evaluate_model_performance(
        source_model, perturbed_test_data, test_labels, 
        config['batch_size'], "Source_Model_A_Perturbed"
    )
    
    source_comparison = evaluator.compare_performance(source_original, source_perturbed)
    
    print(f"ğŸ“Š æºæ¨¡å‹Aæ€§èƒ½å¯¹æ¯”:")
    print(f"   åŸå§‹å‡†ç¡®ç‡: {source_original['accuracy']:.4f}")
    print(f"   æ‰°åŠ¨åå‡†ç¡®ç‡: {source_perturbed['accuracy']:.4f}")
    print(f"   å‡†ç¡®ç‡ä¸‹é™: {source_comparison['accuracy_drop']:.4f}")
    print(f"   æ”»å‡»æˆåŠŸç‡: {source_comparison['attack_success_rate']:.4f}")
    
    # ==================== æ­¥éª¤3: åœ¨ç›®æ ‡æ¨¡å‹Bç­‰ä¸ŠéªŒè¯è¿ç§»æ•ˆæœ ====================
    print(f"\nğŸ”„ æ­¥éª¤3: åœ¨ç›®æ ‡æ¨¡å‹ä¸ŠéªŒè¯è¿ç§»æ”»å‡»æ•ˆæœ...")
    print("-" * 30)
    
    target_results = {}
    
    for model_name, model in target_models.items():
        print(f"\nğŸ“‹ è¯„ä¼°ç›®æ ‡æ¨¡å‹: {model_name}")
        
        # åŸå§‹æ€§èƒ½
        original_perf = evaluator.evaluate_model_performance(
            model, test_data, test_labels, 
            config['batch_size'], model_name
        )
        
        # æ‰°åŠ¨åæ€§èƒ½
        perturbed_perf = evaluator.evaluate_model_performance(
            model, perturbed_test_data, test_labels, 
            config['batch_size'], f"{model_name}_Perturbed"
        )
        
        # æ€§èƒ½å¯¹æ¯”
        comparison = evaluator.compare_performance(original_perf, perturbed_perf)
        
        target_results[model_name] = {
            'original': original_perf,
            'perturbed': perturbed_perf,
            'comparison': comparison
        }
        
        print(f"   åŸå§‹å‡†ç¡®ç‡: {original_perf['accuracy']:.4f}")
        print(f"   æ‰°åŠ¨åå‡†ç¡®ç‡: {perturbed_perf['accuracy']:.4f}")
        print(f"   å‡†ç¡®ç‡ä¸‹é™: {comparison['accuracy_drop']:.4f}")
        print(f"   æ”»å‡»æˆåŠŸç‡: {comparison['attack_success_rate']:.4f}")
        print(f"   è¿ç§»æ•ˆæœ: {'âœ… è‰¯å¥½' if comparison['accuracy_drop'] > 0.1 else 'âš ï¸ ä¸€èˆ¬' if comparison['accuracy_drop'] > 0.05 else 'âŒ è¾ƒå·®'}")
    
    # ==================== æ­¥éª¤4: æ±‡æ€»ç»“æœ ====================
    print(f"\nğŸ“ˆ æ­¥éª¤4: æ±‡æ€»å®éªŒç»“æœ...")
    print("=" * 50)
    
    # æ•´ç†æ‰€æœ‰ç»“æœ
    final_results = {
        'experiment_info': {
            'timestamp': timestamp,
            'config': config,
            'device': str(device)
        },
        'perturbation_info': perturbation_info,
        'source_model': {
            'original': source_original,
            'perturbed': source_perturbed,
            'comparison': source_comparison
        },
        'target_models': target_results,
        'summary': {
            'avg_accuracy_drop': np.mean([res['comparison']['accuracy_drop'] 
                                        for res in target_results.values()]),
            'avg_attack_success_rate': np.mean([res['comparison']['attack_success_rate'] 
                                              for res in target_results.values()]),
            'best_transfer_model': max(target_results.keys(), 
                                     key=lambda x: target_results[x]['comparison']['accuracy_drop']),
            'worst_transfer_model': min(target_results.keys(), 
                                      key=lambda x: target_results[x]['comparison']['accuracy_drop'])
        }
    }
    
    # æ‰“å°æ€»ç»“
    print(f"ğŸ¯ å®éªŒæ€»ç»“:")
    print(f"   æ‰°åŠ¨ä½ç½®æ•°é‡: {perturbation_info['num_positions']}")
    print(f"   æ‰°åŠ¨æ¯”ä¾‹: {perturbation_info['perturbation_ratio']:.2%}")
    print(f"   æºæ¨¡å‹æ”»å‡»æˆåŠŸç‡: {source_comparison['attack_success_rate']:.4f}")
    print(f"   å¹³å‡è¿ç§»æ”»å‡»æˆåŠŸç‡: {final_results['summary']['avg_attack_success_rate']:.4f}")
    print(f"   æœ€ä½³è¿ç§»æ¨¡å‹: {final_results['summary']['best_transfer_model']}")
    print(f"   æœ€å·®è¿ç§»æ¨¡å‹: {final_results['summary']['worst_transfer_model']}")
    
    # ==================== æ­¥éª¤5: ä¿å­˜ç»“æœ ====================
    print(f"\nğŸ’¾ æ­¥éª¤5: ä¿å­˜å®éªŒç»“æœ...")
    
    # ä¿å­˜JSONç»“æœ
    results_path = os.path.join(output_dir, 'experiment_results.json')
    save_results(final_results, results_path)
    
    # ä¿å­˜æ‰°åŠ¨ä½ç½®
    perturbation_path = os.path.join(output_dir, 'universal_perturbation.pt')
    torch.save({
        'positions': best_positions,
        'fitness': best_fitness,
        'config': config['haad_config']
    }, perturbation_path)
    print(f"âœ… é€šç”¨æ‰°åŠ¨å·²ä¿å­˜åˆ°: {perturbation_path}")
    
    # ä¿å­˜é…ç½®
    config_path = os.path.join(output_dir, 'experiment_config.json')
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2)
    print(f"âœ… å®éªŒé…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    if config['save_plots']:
        print(f"\n, ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        plot_results(final_results, output_dir)
    
    print(f"\nğŸ‰ å®éªŒå®Œæˆ! æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    print("=" * 50)
    
    return final_results


if __name__ == "__main__":
    results = main()